{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qsBTL0O1-5r",
        "outputId": "f95c9625-596a-44c4-c46b-d0eea71b842b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Input, Dropout\n",
        "from keras.layers import Dense\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from keras.models import Model\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataProcessing"
      ],
      "metadata": {
        "id": "RLhPjapiQb5w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Pv2pkR6F2MHl"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def prepare_data_ae(original_data):\n",
        "    distinct_activities = original_data['Type of economic activity'].unique()\n",
        "    original_data['Consumption per meter (L/Day)'] = original_data['Consumption (L/Day)'] /original_data['Number of meters']\n",
        "    columns_to_drop = ['Consumption (L/Day)', 'Number of meters']\n",
        "    original_data = original_data.drop(columns=columns_to_drop)\n",
        "\n",
        "    # Group the DataFrame by 'District', 'Use', and 'Type of economic activity'\n",
        "    grouped_data = original_data.groupby(['Postcode', 'Use', 'Type of economic activity'])\n",
        "\n",
        "    postcode_activity_dataframes = {}\n",
        "    reshaped_data_list = []\n",
        "    for (postcode, use, activity), data in grouped_data:\n",
        "        key = f'{postcode}_{use}_{activity}'\n",
        "        postcode, use, activity = key.split('_')\n",
        "\n",
        "        postcode_activity_dataframes[key] = data.copy()\n",
        "        #postcode_activity_dataframes[key]['Date'] = pd.to_datetime(postcode_activity_dataframes[key][['Year', 'Month', 'Day']])\n",
        "        postcode_activity_dataframes[key] = postcode_activity_dataframes[key].sort_values(by='Date', ascending=True)\n",
        "\n",
        "        label_encoder = LabelEncoder()\n",
        "\n",
        "        df_encoded = postcode_activity_dataframes[key]\n",
        "\n",
        "        df_encoded['Postcode'] = label_encoder.fit_transform(df_encoded['Postcode'])\n",
        "        df_encoded['Use'] = label_encoder.fit_transform(df_encoded['Use'])\n",
        "        df_encoded['Type of economic activity'] = label_encoder.fit_transform(df_encoded['Type of economic activity'])\n",
        "\n",
        "        df_encoded.reset_index(inplace=True)\n",
        "        df_encoded = df_encoded.groupby('Date').agg({\n",
        "            'Postcode': 'first',\n",
        "            'Use': 'first',\n",
        "            'Type of economic activity':'first',\n",
        "            'Consumption per meter (L/Day)': 'max',\n",
        "            # 'Precipitaciones (mm)': 'mean',\n",
        "            #\n",
        "            'Date' : 'first',\n",
        "            # 'T_max (C)': 'mean',\n",
        "            # 'T_min (C)': 'mean',\n",
        "            'spi_12': 'mean',\n",
        "            'spi_9': 'mean',\n",
        "            'scpdsi': 'mean'\n",
        "        }).reset_index(drop=True)\n",
        "\n",
        "        data = df_encoded.values\n",
        "        postcode_activity_dataframes[key] = df_encoded\n",
        "        if (data.shape[0]) == 1460 :\n",
        "            reshaped_data_list.append(data)\n",
        "\n",
        "    reshaped_data = np.stack(reshaped_data_list, axis=0)\n",
        "\n",
        "    return reshaped_data, postcode_activity_dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bbR7UwWL3GcZ"
      },
      "outputs": [],
      "source": [
        "path = '/content/drive/MyDrive/Project Management/AB Data/Datasets limpios/Datasets_Barcelona/abdataset1_rest_seq_v2.csv'\n",
        "original_data = pd.read_csv(path)\n",
        "original_data['Type of economic activity'] = original_data['Type of economic activity']\n",
        "tensor_data, dataframes = prepare_data_ae(original_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tensor_data.shape)\n",
        "print(tensor_data[0][2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjBvLwbzKtfj",
        "outputId": "b9429e9f-2631-4758-fde1-4cde669247d6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1251, 1460, 8)\n",
            "[0 0 0 1223.0 '2019-01-03' 1.59 1.2 2.59]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_without_timestamp = np.delete(tensor_data, 4, axis=-1)\n",
        "tensor_data = data_without_timestamp.astype(float)\n",
        "del data_without_timestamp"
      ],
      "metadata": {
        "id": "Die38rwmMLKX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the sequence length (time window)\n",
        "seq_length = 30  # You can adjust this based on your requirement\n",
        "\n",
        "# Reshape the data into sequences of specified time window\n",
        "def create_sequences(data, seq_length):\n",
        "    sequences = []\n",
        "    num_batches, num_timesteps, num_features = data.shape\n",
        "\n",
        "    for i in range(num_batches):\n",
        "        for j in range(0, num_timesteps - seq_length + 1):\n",
        "            sequence = data[i, j:j+seq_length, :]\n",
        "            sequences.append(sequence)\n",
        "\n",
        "    return np.array(sequences)\n",
        "\n",
        "# Prepare sequences for input to the model\n",
        "sequences = create_sequences(tensor_data, seq_length)\n",
        "\n",
        "# Print the shape of the prepared sequences\n",
        "print(\"Shape of prepared sequences:\", sequences.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Azu5w7HKSJu",
        "outputId": "ba170a41-b8c7-4734-ccc9-ac99be38734b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of prepared sequences: (1790181, 30, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Psdmku4c9Xkg"
      },
      "outputs": [],
      "source": [
        "# Function to create sequences from the 3D tensor\n",
        "#def to_sequences(data, seq_size=1):\n",
        " #   x_values = []\n",
        " #   y_values = []\n",
        "#\n",
        "#    for i in range(len(data) - seq_size):\n",
        " #       x_values.append(data[i:(i + seq_size)])\n",
        " #       y_values.append(data[i + seq_size])\n",
        "\n",
        "  #  return np.array(x_values), np.array(y_values)\n",
        "\n",
        "# Set the sequence size\n",
        "#seq_size = 30  # Number of time steps to look back\n",
        "\n",
        "# Create sequences for LSTM model\n",
        "#x_sequences, y_sequences = [], []\n",
        "#for series in tensor_data:\n",
        "  #  x, y = to_sequences(series, seq_size)\n",
        "  #  x_sequences.append(x)\n",
        "  #  y_sequences.append(y)\n",
        "\n",
        "# Convert the lists to numpy arrays\n",
        "#trainX = np.array(x_sequences)\n",
        "#trainY = np.array(y_sequences)\n",
        "\n",
        "#print(trainX.shape)\n",
        "#print(trainY.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KERAS VAE-LSTM"
      ],
      "metadata": {
        "id": "KCENkXf2PLBC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKB9-rkDNE1F"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "\n",
        "# Define the input shape based on the number of time series, time steps, and features\n",
        "input_shape = (trainX.shape[1], trainX.shape[2], trainX.shape[3])  # (num_sequences, seq_size, num_features)\n",
        "\n",
        "# Define the encoder\n",
        "encoder_inputs = Input(shape=input_shape)\n",
        "encoder_lstm1 = LSTM(128, return_sequences=False)(encoder_inputs)\n",
        "encoder_dropout1 = Dropout(rate=0.2)(encoder_lstm1)\n",
        "\n",
        "# Bottleneck layer\n",
        "encoded = RepeatVector(input_shape[0])(encoder_dropout1)\n",
        "\n",
        "# Define the decoder\n",
        "decoder_lstm1 = LSTM(128, return_sequences=True)(encoded)\n",
        "decoder_dropout1 = Dropout(rate=0.2)(decoder_lstm1)\n",
        "decoder_outputs = TimeDistributed(Dense(input_shape[1]))(decoder_dropout1)\n",
        "\n",
        "# Create the autoencoder model\n",
        "autoencoder = Model(inputs=encoder_inputs, outputs=decoder_outputs)\n",
        "\n",
        "# Compile the model\n",
        "autoencoder.compile(optimizer='adam', loss='mae')\n",
        "autoencoder.summary()\n",
        "\n",
        "# Train the model\n",
        "history = autoencoder.fit(trainX, trainY, epochs=10, batch_size=1, validation_split=0.1, verbose=1)\n",
        "\n",
        "# Plot the training and validation loss\n",
        "plt.plot(history.history['loss'], label='Training loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "1XD8_8bFFpwS",
        "outputId": "ba4ed30c-4ae2-4082-ca4e-f330212bcc85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "45/45 [==============================] - 0s 4ms/step\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainPredict = model.predict(trainX)\n",
        "trainMAE = np.mean(np.abs(trainPredict - trainX), axis=1)\n",
        "plt.hist(trainMAE, bins=30)\n",
        "max_trainMAE = 0.3  #or Define 90% value of max as threshold."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8aWzkohZyVw",
        "outputId": "de7afec5-3386-4351-b4f4-6574ab1fff5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6, 1460, 11)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM-VAE SIMPLE"
      ],
      "metadata": {
        "id": "HmGq6U4oOuYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the LSTM-VAE model\n",
        "class LSTMVAE(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(LSTMVAE, self).__init__()\n",
        "        self.encoder_lstm = nn.LSTM(input_size, hidden_size)\n",
        "        self.decoder_lstm = nn.LSTM(hidden_size, input_size)\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        _, (encoder_hidden, encoder_cell) = self.encoder_lstm(x)\n",
        "\n",
        "        # Bottleneck layer\n",
        "        bottleneck = encoder_hidden.view(-1, 1, self.hidden_size).repeat(x.size(0), 1, 1)\n",
        "\n",
        "        # Decoder\n",
        "        output, _ = self.decoder_lstm(bottleneck)\n",
        "        return output\n",
        "\n",
        "# Define the input size (number of features) and hidden size\n",
        "input_size = sequences.shape[2]  # number of features in each time step\n",
        "hidden_size = 128\n",
        "\n",
        "# Initialize the model\n",
        "lstm_vae = LSTMVAE(input_size, hidden_size)\n",
        "\n",
        "# Define the loss function (Mean Absolute Error)\n",
        "criterion = nn.L1Loss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.Adam(lstm_vae.parameters(), lr=0.001)\n",
        "\n",
        "# Convert sequences to PyTorch tensor\n",
        "sequences_tensor = torch.tensor(sequences, dtype=torch.float32)\n",
        "\n",
        "# Training the model\n",
        "num_epochs = 10\n",
        "history = []\n",
        "for epoch in range(num_epochs):\n",
        "    lstm_vae.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    # Iterate over the sequences in each epoch\n",
        "    for i in range(sequences.shape[0]):\n",
        "        # Extract the current sequence\n",
        "        input_sequence = sequences_tensor[i].unsqueeze(0)  # add batch dimension\n",
        "        print(input_sequence)\n",
        "        # Reset gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        output_sequence = lstm_vae(input_sequence)\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = criterion(output_sequence, input_sequence)\n",
        "\n",
        "        # Backpropagation and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate the total loss for this epoch\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Calculate the average loss for this epoch\n",
        "    average_loss = total_loss / sequences.shape[0]\n",
        "    history.append(average_loss)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {average_loss:.4f}')\n",
        "    if (epoch + 1) % 5 == 0:  # Visualize every 5 epochs (you can adjust this as needed)\n",
        "        with torch.no_grad():\n",
        "            reconstructed_sequence = lstm_vae(input_sequence)\n",
        "\n",
        "            # Convert tensors back to numpy arrays for plotting\n",
        "            input_sequence_np = input_sequence.squeeze().numpy()\n",
        "            reconstructed_sequence_np = reconstructed_sequence.squeeze().numpy()\n",
        "\n",
        "            # Plot input and reconstructed sequences\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            plt.plot(input_sequence_np, label='Input Sequence')\n",
        "            plt.plot(reconstructed_sequence_np, label='Reconstructed Sequence')\n",
        "            plt.xlabel('Time Step')\n",
        "            plt.ylabel('Feature Value')\n",
        "            plt.title('Input Sequence vs. Reconstructed Sequence')\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "\n",
        "# Plotting the training loss\n",
        "plt.plot(range(1, num_epochs+1), history, label='Training loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Y99zUKEiazk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# USAD"
      ],
      "metadata": {
        "id": "CTlmFClsOp_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r sample_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NIUhl-SOpqX",
        "outputId": "e9247718-4788-4dd5-8edb-40f455a663cf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'sample_data': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/manigalati/usad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1fn-0KyPQ0l",
        "outputId": "78cfa641-e983-40fb-857e-44ecce616383"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'usad' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd usad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTEnU3eJPT09",
        "outputId": "1bb6470e-c170-4e0e-bba1-6037c23bb904"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/usad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from utils import *\n",
        "from usad import *"
      ],
      "metadata": {
        "id": "YmbbvBbDPYzk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L\n",
        "\n",
        "device = get_default_device()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLoUX9gvPc-c",
        "outputId": "1287a829-5405-4834-9877-6439e6119760"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-45c86449-8791-8b9f-fe47-ddc101987030)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "window_size=12"
      ],
      "metadata": {
        "id": "HlWaWsuePnny"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#windows_normal=normal.values[np.arange(window_size)[None, :] + np.arange(normal.shape[0]-window_size)[:, None]]\n",
        "\n",
        "sequences_train = sequences[:int(np.floor(.8 *  sequences.shape[0]))]\n",
        "sequences_val = sequences[int(np.floor(.8 *  sequences.shape[0])):int(np.floor(sequences.shape[0]))]\n"
      ],
      "metadata": {
        "id": "TPw5LzK1Pql6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.utils.data as data_utils\n",
        "\n",
        "BATCH_SIZE =  1\n",
        "N_EPOCHS = 1\n",
        "hidden_size = 100\n",
        "\n",
        "w_size=sequences.shape[1]*sequences.shape[2]\n",
        "z_size=sequences.shape[1]*hidden_size\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(data_utils.TensorDataset(\n",
        "    torch.from_numpy(sequences).float().view(([sequences.shape[0],w_size]))\n",
        ") , batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(data_utils.TensorDataset(\n",
        "    torch.from_numpy(sequences_val).float().view(([sequences_val.shape[0],w_size]))\n",
        ") , batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "model = UsadModel(w_size, z_size)\n",
        "model = to_device(model,device)"
      ],
      "metadata": {
        "id": "CWTNnOeqPueU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = training(N_EPOCHS,model,train_loader,val_loader)"
      ],
      "metadata": {
        "id": "aycTH2ZpQsX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_history(history)"
      ],
      "metadata": {
        "id": "QyTrOTc5QvaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save({\n",
        "            'encoder': model.encoder.state_dict(),\n",
        "            'decoder1': model.decoder1.state_dict(),\n",
        "            'decoder2': model.decoder2.state_dict()\n",
        "            }, \"model.pth\")"
      ],
      "metadata": {
        "id": "2d0t0ZRQQzOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing (hace falta ver como son los outputs y tal)"
      ],
      "metadata": {
        "id": "Ig4PauRnRhI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load(\"model.pth\")\n",
        "\n",
        "model.encoder.load_state_dict(checkpoint['encoder'])\n",
        "model.decoder1.load_state_dict(checkpoint['decoder1'])\n",
        "model.decoder2.load_state_dict(checkpoint['decoder2'])"
      ],
      "metadata": {
        "id": "-uqi5SiHRMTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results=testing(model,val_loader)"
      ],
      "metadata": {
        "id": "wYP8bS90Q4uB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results.shape"
      ],
      "metadata": {
        "id": "lOXSvaqsRY8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "windows_labels=[]\n",
        "for i in range(len(labels)-window_size):\n",
        "    windows_labels.append(list(np.int_(labels[i:i+window_size])))"
      ],
      "metadata": {
        "id": "TAQ3jdbaQ8JI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MTGFLOW"
      ],
      "metadata": {
        "id": "W5HUG2wUrXJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.distributions as D\n",
        "import math\n",
        "import copy\n",
        "\n",
        "\n",
        "# --------------------\n",
        "# Model layers and helpers\n",
        "# --------------------\n",
        "\n",
        "def create_masks(input_size, hidden_size, n_hidden, input_order='sequential', input_degrees=None):\n",
        "    # MADE paper sec 4:\n",
        "    # degrees of connections between layers -- ensure at most in_degree - 1 connections\n",
        "    degrees = []\n",
        "\n",
        "    # set input degrees to what is provided in args (the flipped order of the previous layer in a stack of mades);\n",
        "    # else init input degrees based on strategy in input_order (sequential or random)\n",
        "    if input_size>1:\n",
        "        if input_order == 'sequential':\n",
        "            degrees += [torch.arange(input_size)] if input_degrees is None else [input_degrees]\n",
        "            for _ in range(n_hidden + 1):\n",
        "                degrees += [torch.arange(hidden_size) % (input_size - 1)]\n",
        "            degrees += [torch.arange(input_size) % input_size - 1] if input_degrees is None else [input_degrees % input_size - 1]\n",
        "\n",
        "        elif input_order == 'random':\n",
        "            degrees += [torch.randperm(input_size)] if input_degrees is None else [input_degrees]\n",
        "            for _ in range(n_hidden + 1):\n",
        "                min_prev_degree = min(degrees[-1].min().item(), input_size - 1)\n",
        "                degrees += [torch.randint(min_prev_degree, input_size, (hidden_size,))]\n",
        "            min_prev_degree = min(degrees[-1].min().item(), input_size - 1)\n",
        "            degrees += [torch.randint(min_prev_degree, input_size, (input_size,)) - 1] if input_degrees is None else [input_degrees - 1]\n",
        "    else:\n",
        "        degrees += [torch.zeros([1]).long()]\n",
        "        for _ in range(n_hidden+1):\n",
        "            degrees += [torch.zeros([hidden_size]).long()]\n",
        "        degrees += [torch.zeros([input_size]).long()]\n",
        "    # construct masks\n",
        "    masks = []\n",
        "    for (d0, d1) in zip(degrees[:-1], degrees[1:]):\n",
        "        masks += [(d1.unsqueeze(-1) >= d0.unsqueeze(0)).float()]\n",
        "\n",
        "    return masks, degrees[0]\n",
        "\n",
        "#%%\n",
        "\n",
        "def create_masks_pmu(input_size, hidden_size, n_hidden, input_order='sequential', input_degrees=None):\n",
        "    # MADE paper sec 4:\n",
        "    # degrees of connections between layers -- ensure at most in_degree - 1 connections\n",
        "    degrees = []\n",
        "\n",
        "    # set input degrees to what is provided in args (the flipped order of the previous layer in a stack of mades);\n",
        "    # else init input degrees based on strategy in input_order (sequential or random)\n",
        "    if input_order == 'sequential':\n",
        "        degrees += [torch.arange(input_size)] if input_degrees is None else [input_degrees]\n",
        "        for _ in range(n_hidden + 1):\n",
        "            degrees += [torch.arange(hidden_size) % (input_size - 1)]\n",
        "        degrees += [torch.arange(input_size) % input_size - 1] if input_degrees is None else [input_degrees % input_size - 1]\n",
        "\n",
        "    # construct masks\n",
        "    masks = []\n",
        "    for (d0, d1) in zip(degrees[:-1], degrees[1:]):\n",
        "        masks += [(d1.unsqueeze(-1) >= d0.unsqueeze(0)).float()]\n",
        "    masks[0] = masks[0].repeat_interleave(3, dim=1)\n",
        "    masks[-1] = masks[-1].repeat_interleave(3, dim=0)\n",
        "\n",
        "    return masks, degrees[0]\n",
        "#%%\n",
        "class MaskedLinear(nn.Linear):\n",
        "    \"\"\" MADE building block layer \"\"\"\n",
        "    def __init__(self, input_size, n_outputs, mask, cond_label_size=None):\n",
        "        super().__init__(input_size, n_outputs)\n",
        "\n",
        "        self.register_buffer('mask', mask)\n",
        "\n",
        "        self.cond_label_size = cond_label_size\n",
        "        if cond_label_size is not None:\n",
        "            self.cond_weight = nn.Parameter(torch.rand(n_outputs, cond_label_size) / math.sqrt(cond_label_size))\n",
        "\n",
        "    def forward(self, x, y=None):\n",
        "        out = F.linear(x, self.weight * self.mask, self.bias)\n",
        "        if y is not None:\n",
        "            out = out + F.linear(y, self.cond_weight)\n",
        "        return out\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return 'in_features={}, out_features={}, bias={}'.format(\n",
        "            self.in_features, self.out_features, self.bias is not None\n",
        "        ) + (self.cond_label_size != None) * ', cond_features={}'.format(self.cond_label_size)\n",
        "\n",
        "\n",
        "class LinearMaskedCoupling(nn.Module):\n",
        "    \"\"\" Modified RealNVP Coupling Layers per the MAF paper \"\"\"\n",
        "    def __init__(self, input_size, hidden_size, n_hidden, mask, cond_label_size=None):\n",
        "        super().__init__()\n",
        "\n",
        "        self.register_buffer('mask', mask)\n",
        "\n",
        "        # scale function\n",
        "        s_net = [nn.Linear(input_size + (cond_label_size if cond_label_size is not None else 0), hidden_size)]\n",
        "        for _ in range(n_hidden):\n",
        "            s_net += [nn.Tanh(), nn.Linear(hidden_size, hidden_size)]\n",
        "        s_net += [nn.Tanh(), nn.Linear(hidden_size, input_size)]\n",
        "        self.s_net = nn.Sequential(*s_net)\n",
        "\n",
        "        # translation function\n",
        "        self.t_net = copy.deepcopy(self.s_net)\n",
        "        # replace Tanh with ReLU's per MAF paper\n",
        "        for i in range(len(self.t_net)):\n",
        "            if not isinstance(self.t_net[i], nn.Linear): self.t_net[i] = nn.ReLU()\n",
        "\n",
        "    def forward(self, x, y=None):\n",
        "        # apply mask\n",
        "        mx = x * self.mask\n",
        "\n",
        "        # run through model\n",
        "        s = self.s_net(mx if y is None else torch.cat([y, mx], dim=1))\n",
        "        t = self.t_net(mx if y is None else torch.cat([y, mx], dim=1))\n",
        "        u = mx + (1 - self.mask) * (x - t) * torch.exp(-s)  # cf RealNVP eq 8 where u corresponds to x (here we're modeling u)\n",
        "\n",
        "        log_abs_det_jacobian = - (1 - self.mask) * s  # log det du/dx; cf RealNVP 8 and 6; note, sum over input_size done at model log_prob\n",
        "\n",
        "        return u, log_abs_det_jacobian\n",
        "\n",
        "    def inverse(self, u, y=None):\n",
        "        # apply mask\n",
        "        mu = u * self.mask\n",
        "\n",
        "        # run through model\n",
        "        s = self.s_net(mu if y is None else torch.cat([y, mu], dim=1))\n",
        "        t = self.t_net(mu if y is None else torch.cat([y, mu], dim=1))\n",
        "        x = mu + (1 - self.mask) * (u * s.exp() + t)  # cf RealNVP eq 7\n",
        "\n",
        "        log_abs_det_jacobian = (1 - self.mask) * s  # log det dx/du\n",
        "\n",
        "        return x, log_abs_det_jacobian\n",
        "\n",
        "\n",
        "class BatchNorm(nn.Module):\n",
        "    \"\"\" RealNVP BatchNorm layer \"\"\"\n",
        "    def __init__(self, input_size, momentum=0.9, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.momentum = momentum\n",
        "        self.eps = eps\n",
        "\n",
        "        self.log_gamma = nn.Parameter(torch.zeros(input_size))\n",
        "        self.beta = nn.Parameter(torch.zeros(input_size))\n",
        "\n",
        "        self.register_buffer('running_mean', torch.zeros(input_size))\n",
        "        self.register_buffer('running_var', torch.ones(input_size))\n",
        "\n",
        "    def forward(self, x, cond_y=None):\n",
        "        if self.training:\n",
        "            self.batch_mean = x.mean(0)\n",
        "            self.batch_var = x.var(0) # note MAF paper uses biased variance estimate; ie x.var(0, unbiased=False)\n",
        "\n",
        "            # update running mean\n",
        "            self.running_mean.mul_(self.momentum).add_(self.batch_mean.data * (1 - self.momentum))\n",
        "            self.running_var.mul_(self.momentum).add_(self.batch_var.data * (1 - self.momentum))\n",
        "\n",
        "            mean = self.batch_mean\n",
        "            var = self.batch_var\n",
        "        else:\n",
        "            mean = self.running_mean\n",
        "            var = self.running_var\n",
        "\n",
        "        # compute normalized input (cf original batch norm paper algo 1)\n",
        "        x_hat = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        y = self.log_gamma.exp() * x_hat + self.beta\n",
        "\n",
        "        # compute log_abs_det_jacobian (cf RealNVP paper)\n",
        "        log_abs_det_jacobian = self.log_gamma - 0.5 * torch.log(var + self.eps)\n",
        "#        print('in sum log var {:6.3f} ; out sum log var {:6.3f}; sum log det {:8.3f}; mean log_gamma {:5.3f}; mean beta {:5.3f}'.format(\n",
        "#            (var + self.eps).log().sum().data.numpy(), y.var(0).log().sum().data.numpy(), log_abs_det_jacobian.mean(0).item(), self.log_gamma.mean(), self.beta.mean()))\n",
        "        return y, log_abs_det_jacobian.expand_as(x)\n",
        "\n",
        "    def inverse(self, y, cond_y=None):\n",
        "        if self.training:\n",
        "            mean = self.batch_mean\n",
        "            var = self.batch_var\n",
        "        else:\n",
        "            mean = self.running_mean\n",
        "            var = self.running_var\n",
        "\n",
        "        x_hat = (y - self.beta) * torch.exp(-self.log_gamma)\n",
        "        x = x_hat * torch.sqrt(var + self.eps) + mean\n",
        "\n",
        "        log_abs_det_jacobian = 0.5 * torch.log(var + self.eps) - self.log_gamma\n",
        "\n",
        "        return x, log_abs_det_jacobian.expand_as(x)\n",
        "\n",
        "\n",
        "class FlowSequential(nn.Sequential):\n",
        "    \"\"\" Container for layers of a normalizing flow \"\"\"\n",
        "    def forward(self, x, y):\n",
        "        sum_log_abs_det_jacobians = 0\n",
        "        for module in self:\n",
        "            x, log_abs_det_jacobian = module(x, y)\n",
        "            sum_log_abs_det_jacobians = sum_log_abs_det_jacobians + log_abs_det_jacobian\n",
        "        return x, sum_log_abs_det_jacobians\n",
        "\n",
        "    def inverse(self, u, y):\n",
        "        sum_log_abs_det_jacobians = 0\n",
        "        for module in reversed(self):\n",
        "            u, log_abs_det_jacobian = module.inverse(u, y)\n",
        "            sum_log_abs_det_jacobians = sum_log_abs_det_jacobians + log_abs_det_jacobian\n",
        "        return u, sum_log_abs_det_jacobians\n",
        "\n",
        "# --------------------\n",
        "# Models\n",
        "# --------------------\n",
        "\n",
        "class MADE(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, n_hidden, cond_label_size=None, activation='relu', input_order='sequential', input_degrees=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_size -- scalar; dim of inputs\n",
        "            hidden_size -- scalar; dim of hidden layers\n",
        "            n_hidden -- scalar; number of hidden layers\n",
        "            activation -- str; activation function to use\n",
        "            input_order -- str or tensor; variable order for creating the autoregressive masks (sequential|random)\n",
        "                            or the order flipped from the previous layer in a stack of mades\n",
        "            conditional -- bool; whether model is conditional\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # base distribution for calculation of log prob under the model\n",
        "        self.register_buffer('base_dist_mean', torch.zeros(input_size))\n",
        "        self.register_buffer('base_dist_var', torch.ones(input_size))\n",
        "\n",
        "        # create masks\n",
        "        masks, self.input_degrees = create_masks(input_size, hidden_size, n_hidden, input_order, input_degrees)\n",
        "\n",
        "        # setup activation\n",
        "        if activation == 'relu':\n",
        "            activation_fn = nn.ReLU()\n",
        "        elif activation == 'tanh':\n",
        "            activation_fn = nn.Tanh()\n",
        "        else:\n",
        "            raise ValueError('Check activation function.')\n",
        "\n",
        "        # construct model\n",
        "        self.net_input = MaskedLinear(input_size, hidden_size, masks[0], cond_label_size)\n",
        "        self.net = []\n",
        "        for m in masks[1:-1]:\n",
        "            self.net += [activation_fn, MaskedLinear(hidden_size, hidden_size, m)]\n",
        "        self.net += [activation_fn, MaskedLinear(hidden_size, 2 * input_size, masks[-1].repeat(2,1))]\n",
        "        self.net = nn.Sequential(*self.net)\n",
        "\n",
        "    @property\n",
        "    def base_dist(self):\n",
        "        return D.Normal(self.base_dist_mean, self.base_dist_var)\n",
        "\n",
        "    def forward(self, x, y=None):\n",
        "        # MAF eq 4 -- return mean and log std\n",
        "        m, loga = self.net(self.net_input(x, y)).chunk(chunks=2, dim=1)\n",
        "        u = (x - m) * torch.exp(-loga)\n",
        "        # MAF eq 5\n",
        "        log_abs_det_jacobian = - loga\n",
        "        return u, log_abs_det_jacobian\n",
        "\n",
        "    def inverse(self, u, y=None, sum_log_abs_det_jacobians=None):\n",
        "        # MAF eq 3\n",
        "        D = u.shape[1]\n",
        "        x = torch.zeros_like(u)\n",
        "        # run through reverse model\n",
        "        for i in self.input_degrees:\n",
        "            m, loga = self.net(self.net_input(x, y)).chunk(chunks=2, dim=1)\n",
        "            x[:,i] = u[:,i] * torch.exp(loga[:,i]) + m[:,i]\n",
        "        log_abs_det_jacobian = loga\n",
        "        return x, log_abs_det_jacobian\n",
        "\n",
        "    def log_prob(self, x, y=None):\n",
        "        u, log_abs_det_jacobian = self.forward(x, y)\n",
        "        return torch.sum(self.base_dist.log_prob(u) + log_abs_det_jacobian, dim=1)\n",
        "\n",
        "\n",
        "class MADE_Full(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, n_hidden, cond_label_size=None, activation='relu', input_order='sequential', input_degrees=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_size -- scalar; dim of inputs\n",
        "            hidden_size -- scalar; dim of hidden layers\n",
        "            n_hidden -- scalar; number of hidden layers\n",
        "            activation -- str; activation function to use\n",
        "            input_order -- str or tensor; variable order for creating the autoregressive masks (sequential|random)\n",
        "                            or the order flipped from the previous layer in a stack of mades\n",
        "            conditional -- bool; whether model is conditional\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # base distribution for calculation of log prob under the model\n",
        "        self.register_buffer('base_dist_mean', torch.zeros(input_size))\n",
        "        self.register_buffer('base_dist_var', torch.ones(input_size))\n",
        "\n",
        "        # create masks\n",
        "        masks, self.input_degrees = create_masks_pmu(int(input_size/3), hidden_size, n_hidden, input_order, input_degrees)\n",
        "\n",
        "        # setup activation\n",
        "        if activation == 'relu':\n",
        "            activation_fn = nn.ReLU()\n",
        "        elif activation == 'tanh':\n",
        "            activation_fn = nn.Tanh()\n",
        "        else:\n",
        "            raise ValueError('Check activation function.')\n",
        "\n",
        "        # construct model\n",
        "        self.net_input = MaskedLinear(input_size, hidden_size, masks[0], cond_label_size)\n",
        "        self.net = []\n",
        "        for m in masks[1:-1]:\n",
        "            self.net += [activation_fn, MaskedLinear(hidden_size, hidden_size, m)]\n",
        "        self.net += [activation_fn, MaskedLinear(hidden_size, 2 * input_size, masks[-1].repeat(2,1))]\n",
        "        self.net = nn.Sequential(*self.net)\n",
        "\n",
        "    @property\n",
        "    def base_dist(self):\n",
        "        return D.Normal(self.base_dist_mean, self.base_dist_var)\n",
        "\n",
        "    def forward(self, x, y=None):\n",
        "        # MAF eq 4 -- return mean and log std\n",
        "        m, loga = self.net(self.net_input(x, y)).chunk(chunks=2, dim=1)\n",
        "        u = (x - m) * torch.exp(-loga)\n",
        "        # MAF eq 5\n",
        "        log_abs_det_jacobian = - loga\n",
        "        return u, log_abs_det_jacobian\n",
        "\n",
        "    def log_prob(self, x, y=None):\n",
        "        u, log_abs_det_jacobian = self.forward(x, y)\n",
        "        return torch.sum(self.base_dist.log_prob(u) + log_abs_det_jacobian, dim=1)\n",
        "\n",
        "import numpy as np\n",
        "class MAF(nn.Module):\n",
        "    def __init__(self, n_blocks, n_sensor, input_size, hidden_size, n_hidden, cond_label_size=None, activation='relu', input_order='sequential', batch_norm=True, mode = 'rand'):\n",
        "        super().__init__()\n",
        "        # base distribution for calculation of log prob under the model\n",
        "        if mode == 'zero':\n",
        "            self.register_buffer('base_dist_mean', torch.zeros(n_sensor, 1))\n",
        "            self.register_buffer('base_dist_var', torch.ones(n_sensor, 1))\n",
        "        elif mode == 'rand':\n",
        "            self.register_buffer('base_dist_mean', torch.randn(n_sensor, 1))\n",
        "            self.register_buffer('base_dist_var', torch.ones(n_sensor, 1))\n",
        "\n",
        "        else:\n",
        "            raise AttributeError('no choice')\n",
        "        # construct model\n",
        "        modules = []\n",
        "        self.input_size = input_size\n",
        "        self.input_degrees = None\n",
        "        for i in range(n_blocks):\n",
        "            modules += [MADE(input_size, hidden_size, n_hidden, cond_label_size, activation, input_order, self.input_degrees)]\n",
        "            self.input_degrees = modules[-1].input_degrees.flip(0)\n",
        "            modules += batch_norm * [BatchNorm(input_size)]\n",
        "\n",
        "        self.net = FlowSequential(*modules)\n",
        "\n",
        "\n",
        "    def base_dist(self, z, k, window_size):\n",
        "        # C = z.shape[-1]\n",
        "        # C = 51\n",
        "        N =  z.shape[0]//k//window_size\n",
        "        logp = - 0.5*(z-self.base_dist_mean.repeat_interleave(window_size, 0).repeat(N,1))**2\n",
        "\n",
        "        return logp\n",
        "\n",
        "    def forward(self, x, y=None):\n",
        "        return self.net(x, y)\n",
        "\n",
        "    def inverse(self, u, y=None):\n",
        "        return self.net.inverse(u, y)\n",
        "\n",
        "    def log_prob(self, x, k, window_size, y=None):\n",
        "        # print('shape', self.base_dist_mean.shape)\n",
        "        u, sum_log_abs_det_jacobians = self.forward(x, y)\n",
        "        C = u.shape[1]\n",
        "        return torch.sum(self.base_dist(u, k, window_size) + sum_log_abs_det_jacobians, dim=1)+ C * _GCONST_ #,u\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class MAF_Full(nn.Module):\n",
        "    def __init__(self, n_blocks, n_sensor, input_size, hidden_size, n_hidden, cond_label_size=None, activation='relu', input_order='sequential', batch_norm=True):\n",
        "        super().__init__()\n",
        "        # base distribution for calculation of log prob under the model\n",
        "        self.register_buffer('base_dist_mean', torch.zeros(input_size))\n",
        "        self.register_buffer('base_dist_var', torch.ones(input_size))\n",
        "\n",
        "        # construct model\n",
        "        modules = []\n",
        "        self.input_degrees = None\n",
        "        for i in range(n_blocks):\n",
        "            modules += [MADE_Full(input_size, hidden_size, n_hidden, cond_label_size, activation, input_order, self.input_degrees)]\n",
        "            self.input_degrees = modules[-1].input_degrees.flip(0)\n",
        "            modules += batch_norm * [BatchNorm(input_size)]\n",
        "\n",
        "        self.net = FlowSequential(*modules)\n",
        "\n",
        "    @property\n",
        "    def base_dist(self):\n",
        "        return D.Normal(self.base_dist_mean, self.base_dist_var)\n",
        "\n",
        "    def forward(self, x, y=None):\n",
        "        return self.net(x, y)\n",
        "\n",
        "    def inverse(self, u, y=None):\n",
        "        return self.net.inverse(u, y)\n",
        "\n",
        "    def log_prob(self, x, y=None):\n",
        "        u, sum_log_abs_det_jacobians = self.forward(x, y)\n",
        "        return torch.sum(self.base_dist.log_prob(u) + sum_log_abs_det_jacobians, dim=1)\n",
        "\n",
        "_GCONST_ = -0.9189385332046727 # ln(sqrt(2*pi))"
      ],
      "metadata": {
        "id": "afddc-Ccfvrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from cgitb import reset\n",
        "from turtle import forward, shape\n",
        "from numpy import percentile\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "def interpolate(tensor, index, target_size, mode = 'nearest', dim = 0):\n",
        "    print(tensor.shape)\n",
        "    source_length = tensor.shape[dim]\n",
        "    if source_length > target_size:\n",
        "        raise AttributeError('no need to interpolate')\n",
        "    if dim == -1:\n",
        "        new_tensor = torch.zeros((*tensor.shape[:-1], target_size),dtype=tensor.dtype, device=tensor.device)\n",
        "    if dim == 0:\n",
        "        new_tensor = torch.zeros((target_size, *tensor.shape[1:], ),dtype=tensor.dtype, device=tensor.device)\n",
        "    scale = target_size // source_length\n",
        "    reset = target_size % source_length\n",
        "    # if mode == 'nearest':\n",
        "    new_index = index\n",
        "    new_tensor[new_index, :] = tensor\n",
        "    new_tensor[:new_index[0], :] = tensor[0,:].unsqueeze(0)\n",
        "    for i in range(source_length-1):\n",
        "        new_tensor[new_index[i]:new_index[i+1] , :] = tensor[i,:].unsqueeze(0)\n",
        "    new_tensor[new_index[i+1] :,:] = tensor[i+1,:].unsqueeze(0)\n",
        "    return new_tensor\n",
        "\n",
        "class GNN(nn.Module):\n",
        "    \"\"\"\n",
        "    The GNN module applied in GANF\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "\n",
        "        super(GNN, self).__init__()\n",
        "        self.lin_n = nn.Linear(input_size, hidden_size)\n",
        "        self.lin_r = nn.Linear(input_size, hidden_size, bias=False)\n",
        "        self.lin_2 = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, h, A):\n",
        "        ## A: K X K\n",
        "        ## H: N X K  X L X D\n",
        "        # print(h.shape, A.shape)\n",
        "        # h_n = self.lin_n(torch.einsum('nkld,kj->njld',h,A))\n",
        "        # h_n = self.lin_n(torch.einsum('nkld,kj->njld',h,A))\n",
        "        # print(h.shape, A.shape)\n",
        "        h_n = self.lin_n(torch.einsum('nkld,nkj->njld',h,A))\n",
        "        h_r = self.lin_r(h[:,:,:-1])\n",
        "        h_n[:,:,1:] += h_r\n",
        "        h = self.lin_2(F.relu(h_n))\n",
        "\n",
        "        return h\n",
        "\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_attention(data, i, X_label=None, Y_label=None):\n",
        "  '''\n",
        "    Plot the attention model heatmap\n",
        "    Args:\n",
        "      data: attn_matrix with shape [ty, tx], cutted before 'PAD'\n",
        "      X_label: list of size tx, encoder tags\n",
        "      Y_label: list of size ty, decoder tags\n",
        "  '''\n",
        "  fig, ax = plt.subplots(figsize=(20, 8)) # set figure size\n",
        "  heatmap = ax.pcolor(data, cmap=plt.cm.Blues, alpha=0.9)\n",
        "  fig.colorbar(heatmap)\n",
        "  # Set axis labels\n",
        "  if X_label != None and Y_label != None:\n",
        "    X_label = [x_label for x_label in X_label]\n",
        "    Y_label = [y_label for y_label in Y_label]\n",
        "\n",
        "    xticks = range(0,len(X_label))\n",
        "    ax.set_xticks(xticks, minor=False) # major ticks\n",
        "    ax.set_xticklabels(X_label, minor = False, rotation=45)   # labels should be 'unicode'\n",
        "\n",
        "    yticks = range(0,len(Y_label))\n",
        "    ax.set_yticks(yticks, minor=False)\n",
        "    ax.set_yticklabels(Y_label[::-1], minor = False)   # labels should be 'unicode'\n",
        "\n",
        "    ax.grid(True)\n",
        "    plt.show()\n",
        "    plt.savefig('graph/attention{:04d}.jpg'.format(i))\n",
        "\n",
        "\n",
        "\n",
        "class ScaleDotProductAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    compute scale dot product attention\n",
        "\n",
        "    Query : given sentence that we focused on (decoder)\n",
        "    Key : every sentence to check relationship with Qeury(encoder)\n",
        "    Value : every sentence same with Key (encoder)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, c):\n",
        "        super(ScaleDotProductAttention, self).__init__()\n",
        "        self.w_q = nn.Linear(c, c)\n",
        "        self.w_k = nn.Linear(c, c)\n",
        "        self.w_v = nn.Linear(c, c)\n",
        "        self.softmax = nn.Softmax(dim = 1)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        # swat_0.2\n",
        "    def forward(self, x,mask=None, e=1e-12):\n",
        "        # input is 4 dimension tensor\n",
        "        # [batch_size, head, length, d_tensor]\n",
        "        shape = x.shape\n",
        "        x_shape = x.reshape((shape[0],shape[1], -1))\n",
        "        batch_size, length, c = x_shape.size()\n",
        "        q = self.w_q(x_shape)\n",
        "        k = self.w_k(x_shape)\n",
        "        k_t = k.view(batch_size, c, length)  # transpose\n",
        "        score = (q @ k_t) / math.sqrt(c)  # scaled dot product\n",
        "\n",
        "        # 2. apply masking (opt)\n",
        "        if mask is not None:\n",
        "            score = score.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        # 3. pass them softmax to make [0, 1] range\n",
        "        score = self.dropout(self.softmax(score))\n",
        "\n",
        "\n",
        "\n",
        "        return score, k\n",
        "\n",
        "\n",
        "class MTGFLOW(nn.Module):\n",
        "\n",
        "    def __init__ (self, n_blocks, input_size, hidden_size, n_hidden, window_size, n_sensor, dropout = 0.1, model=\"MAF\", batch_norm=True):\n",
        "        super(MTGFLOW, self).__init__()\n",
        "\n",
        "        self.rnn = nn.LSTM(input_size=input_size,hidden_size=hidden_size,batch_first=True, dropout=dropout)\n",
        "        self.gcn = GNN(input_size=hidden_size, hidden_size=hidden_size)\n",
        "        if model==\"MAF\":\n",
        "            # self.nf = MAF(n_blocks, n_sensor, input_size, hidden_size, n_hidden, cond_label_size=hidden_size, batch_norm=batch_norm,activation='tanh', mode = 'zero')\n",
        "            self.nf = MAF(n_blocks, n_sensor, input_size, hidden_size, n_hidden, cond_label_size=hidden_size, batch_norm=batch_norm,activation='tanh')\n",
        "\n",
        "        self.attention = ScaleDotProductAttention(window_size*input_size)\n",
        "    def forward(self, x, ):\n",
        "\n",
        "        return self.test(x, ).mean()\n",
        "\n",
        "    def test(self, x, ):\n",
        "        # x: N X K X L X D\n",
        "        full_shape = x.shape\n",
        "        graph,_ = self.attention(x)\n",
        "        self.graph = graph\n",
        "        # reshape: N*K, L, D\n",
        "        x = x.reshape((x.shape[0]*x.shape[1], x.shape[2], x.shape[3]))\n",
        "        h,_ = self.rnn(x)\n",
        "\n",
        "        # resahpe: N, K, L, H\n",
        "        h = h.reshape((full_shape[0], full_shape[1], h.shape[1], h.shape[2]))\n",
        "        h = self.gcn(h, graph)\n",
        "\n",
        "        # reshappe N*K*L,H\n",
        "        h = h.reshape((-1,h.shape[3]))\n",
        "        x = x.reshape((-1,full_shape[3]))\n",
        "        log_prob = self.nf.log_prob(x, full_shape[1], full_shape[2], h).reshape([full_shape[0],-1])#\n",
        "        log_prob = log_prob.mean(dim=1)\n",
        "\n",
        "        return log_prob\n",
        "\n",
        "    def get_graph(self):\n",
        "        return self.graph\n",
        "\n",
        "    def locate(self, x, ):\n",
        "        # x: N X K X L X D\n",
        "        full_shape = x.shape\n",
        "\n",
        "        graph, _ = self.attention(x)\n",
        "        # reshape: N*K, L, D\n",
        "        self.graph = graph\n",
        "        x = x.reshape((x.shape[0]*x.shape[1], x.shape[2], x.shape[3]))\n",
        "        h,_ = self.rnn(x)\n",
        "\n",
        "        # resahpe: N, K, L, H\n",
        "        h = h.reshape((full_shape[0], full_shape[1], h.shape[1], h.shape[2]))\n",
        "        h = self.gcn(h, graph)\n",
        "\n",
        "        # reshappe N*K*L,H\n",
        "        h = h.reshape((-1,h.shape[3]))\n",
        "        x = x.reshape((-1,full_shape[3]))\n",
        "        a = self.nf.log_prob(x, full_shape[1], full_shape[2], h)\n",
        "        log_prob, z = a[0].reshape([full_shape[0],full_shape[1],-1]), a[1].reshape([full_shape[0],full_shape[1],-1])\n",
        "\n",
        "\n",
        "\n",
        "        return log_prob.mean(dim=2), z.reshape((full_shape[0]* full_shape[1],-1))\n",
        "\n",
        "\n",
        "class test(nn.Module):\n",
        "    def __init__ (self, n_blocks, input_size, hidden_size, n_hidden, window_size, n_sensor, dropout = 0.1, model=\"MAF\", batch_norm=True):\n",
        "        super(test, self).__init__()\n",
        "\n",
        "        if model==\"MAF\":\n",
        "            self.nf = MAF(n_blocks, n_sensor, input_size, hidden_size, n_hidden, batch_norm=batch_norm,activation='tanh', mode='zero')\n",
        "        self.attention = ScaleDotProductAttention(window_size*input_size)\n",
        "    def forward(self, x, ):\n",
        "        return self.test(x, ).mean()\n",
        "    def test(self, x):\n",
        "        x = x.unsqueeze(2).unsqueeze(3)\n",
        "        full_shape = x.shape\n",
        "        x = x.reshape((full_shape[0]*full_shape[1], full_shape[2], full_shape[3]))\n",
        "        x = x.reshape((-1,full_shape[3]))\n",
        "        log_prob = self.nf.log_prob(x, full_shape[1], full_shape[2]).reshape([full_shape[0],full_shape[1],-1])#*full_shape[1]*full_shape[2]\n",
        "        log_prob = log_prob.mean(dim=1)\n",
        "        return log_prob\n",
        "\n",
        "    def locate(self, x, ):\n",
        "        # x: N X K X L X D\n",
        "        x = x.unsqueeze(2).unsqueeze(3)\n",
        "        full_shape = x.shape\n",
        "\n",
        "        x = x.reshape((x.shape[0]*x.shape[1], x.shape[2], x.shape[3]))\n",
        "\n",
        "        # reshappe N*K*L,H\n",
        "        x = x.reshape((-1,full_shape[3]))\n",
        "        a = self.nf.log_prob(x, full_shape[1], full_shape[2])#*full_shape[1]*full_shape[2]\n",
        "        log_prob, z = a[0].reshape([full_shape[0],full_shape[1],-1]), a[1].reshape([full_shape[0],full_shape[1],-1])\n",
        "\n",
        "\n",
        "        return log_prob.mean(dim=2), z.reshape((full_shape[0]* full_shape[1],-1))"
      ],
      "metadata": {
        "id": "MrW6_YNAfogK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import argparse\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score, precision_recall_curve\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "parser.add_argument('--data_dir', type=str,\n",
        "                    default='Data/input/SWaT_Dataset_Attack_v0.csv', help='Location of datasets.')\n",
        "parser.add_argument('--output_dir', type=str,\n",
        "                    default='./checkpoint/')\n",
        "parser.add_argument('--name',default='SWaT', help='the name of dataset')\n",
        "\n",
        "parser.add_argument('--graph', type=str, default='None')\n",
        "parser.add_argument('--model', type=str, default='MAF')\n",
        "\n",
        "\n",
        "parser.add_argument('--n_blocks', type=int, default=1, help='Number of blocks to stack in a model (MADE in MAF; Coupling+BN in RealNVP).')\n",
        "parser.add_argument('--n_components', type=int, default=1, help='Number of Gaussian clusters for mixture of gaussians models.')\n",
        "parser.add_argument('--hidden_size', type=int, default=32, help='Hidden layer size for MADE (and each MADE block in an MAF).')\n",
        "parser.add_argument('--n_hidden', type=int, default=1, help='Number of hidden layers in each MADE.')\n",
        "parser.add_argument('--input_size', type=int, default=1)\n",
        "parser.add_argument('--batch_norm', type=bool, default=False)\n",
        "parser.add_argument('--train_split', type=float, default=1)\n",
        "parser.add_argument('--stride_size', type=int, default=10)\n",
        "\n",
        "parser.add_argument('--batch_size', type=int, default=512)\n",
        "parser.add_argument('--weight_decay', type=float, default=5e-4)\n",
        "parser.add_argument('--window_size', type=int, default=60)\n",
        "parser.add_argument('--lr', type=float, default=2e-3, help='Learning rate.')\n",
        "\n",
        "\n",
        "\n",
        "args = parser.parse_known_args()[0]\n",
        "args.cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "\n",
        "\n",
        "#for seed in range(15,20):\n",
        "args.seed = 17\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "random.seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "print(\"Loading dataset\")\n",
        "print(args.name)\n",
        "\n",
        "train_loader = 'loader'\n",
        "\n",
        "model = MTGFLOW(args.n_blocks, args.input_size, args.hidden_size, args.n_hidden, args.window_size, 6, dropout=0.0, model = args.model, batch_norm=args.batch_norm)\n",
        "model = model.to(device)\n",
        "\n",
        "from torch.nn.utils import clip_grad_value_\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "save_path = os.path.join(args.output_dir,args.name)\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "\n",
        "\n",
        "loss_best = 100\n",
        "roc_max = 0\n",
        "\n",
        "lr = args.lr\n",
        "optimizer = torch.optim.Adam([\n",
        "    {'params':model.parameters(), 'weight_decay':args.weight_decay},\n",
        "    ], lr=lr, weight_decay=0.0)\n",
        "\n",
        "for epoch in range(40):\n",
        "    print(epoch)\n",
        "    loss_train = []\n",
        "\n",
        "    model.train()\n",
        "    for x in tensor_data:\n",
        "        x = torch.from_numpy(x)\n",
        "        x = x.to(device).float()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = -model(x,)\n",
        "\n",
        "        total_loss = loss\n",
        "\n",
        "        total_loss.backward()\n",
        "        clip_grad_value_(model.parameters(), 1)\n",
        "        optimizer.step()\n",
        "        loss_train.append(loss.item())\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "ZYygJBBigJR6",
        "outputId": "daeddc4c-1757-43e5-ee63-263d738f2206"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset\n",
            "SWaT\n",
            "0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-780da1197486>\u001b[0m in \u001b[0;36m<cell line: 73>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-27fcb91cf5b9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-27fcb91cf5b9>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;31m# x: N X K X L X D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mfull_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;31m# reshape: N*K, L, D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-27fcb91cf5b9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask, e)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mx_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_k\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mk_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# transpose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16060x1 and 60x60)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x in tensor_data:\n",
        "  print(x.shape)\n",
        "  print(x)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7QopEa6iAQY",
        "outputId": "c67a3dcd-70cc-4ab4-a715-8fb4e707ac5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1460, 11)\n",
            "[[ 0.    0.    0.   ...  1.59  1.2   2.59]\n",
            " [ 0.    0.    0.   ...  1.59  1.2   2.59]\n",
            " [ 0.    0.    0.   ...  1.59  1.2   2.59]\n",
            " ...\n",
            " [ 0.    0.    0.   ... -1.32 -1.57 -2.98]\n",
            " [ 0.    0.    0.   ... -1.32 -1.57 -2.98]\n",
            " [ 0.    0.    0.   ... -1.32 -1.57 -2.98]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    loss_test = []\n",
        "    with torch.no_grad():\n",
        "        for x,_,idx in test_loader:\n",
        "\n",
        "            x = x.to(device)\n",
        "            loss = -model.test(x, ).cpu().numpy()\n",
        "            loss_test.append(loss)\n",
        "    loss_test = np.concatenate(loss_test)\n",
        "\n",
        "\n",
        "    roc_test = roc_auc_score(np.asarray(test_loader.dataset.label,dtype=int),loss_test)\n",
        "\n",
        "\n",
        "    if roc_max < roc_test:\n",
        "        roc_max = roc_test\n",
        "        torch.save({\n",
        "        'model': model.state_dict(),\n",
        "        }, f\"{save_path}/model.pth\")\n",
        "\n",
        "    roc_max = max(roc_test, roc_max)\n",
        "    print(roc_max)"
      ],
      "metadata": {
        "id": "5XdHm1B6h7QJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OmniAnomaly"
      ],
      "metadata": {
        "id": "avYH5cmrreCg"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}